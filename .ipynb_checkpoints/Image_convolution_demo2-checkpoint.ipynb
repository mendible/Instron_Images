{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPython (Jupyter) widgets: An image convolution demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution is one of the fundamental concepts of image processing (and more generally, signal processing). For the [`scikit-image` tutorial at Scipy 2014](http://tonysyu.github.io/scikit-image-tutorial-at-scipy-2014.html), I created an IPython widget to help visualize convolution. This post explains that widget in more detail.\n",
    "\n",
    "Very little of this post is actually about using the widget API: The [IPython notebook widgets](http://nbviewer.ipython.org/github/ipython/ipython/blob/master/examples/Interactive%20Widgets/Index.ipynb) have a really easy-to-use API, so only a small bit of code is necessary. That said, I think this makes for a really nice demo of both image convolution and the usefulness of IPython widgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want run this notebook ([Download Notebook](http://tonysyu.github.io/includes/Image_convolution_demo.ipynb)), you'll need:\n",
    "\n",
    "- [IPython](http://ipython.org/) >= 2.0\n",
    "- [matplotlib](http://matplotlib.org/) >= 1.3 (earlier versions probably work)\n",
    "- [scikit-image](http://scikit-image.org/) >= 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside about plotting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started, let's define a bit of boilerplate that's useful for any IPython notebook dealing with images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I *highly* recommended setting the default colormap to 'gray' for images and pretty much everything else. (There are, however, exceptions, as you'll see below.) Also, using nearest neighbor interpolation (which is what 'none' does for zoomed-in images) makes pixel boundaries clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of image convolution is that you take an image like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'filter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6c3c5384bef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcamera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Ignore the Gaussian filter, for now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# (This is explained at the end of the article.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'filter'"
     ]
    }
   ],
   "source": [
    "from skimage import data, filter\n",
    "\n",
    "image = data.camera()\n",
    "# Ignore the Gaussian filter, for now.\n",
    "# (This is explained at the end of the article.)\n",
    "smooth_image = filter.gaussian_filter(image, 5)\n",
    "plt.imshow(smooth_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and filter the image using a convolution \"kernel\" that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "horizontal_edge_kernel = np.array([[ 1,  2,  1],\n",
    "                                   [ 0,  0,  0],\n",
    "                                   [-1, -2, -1]])\n",
    "# Non-gray colormap to color negative values (red) and positive values (blue)\n",
    "plt.imshow(horizontal_edge_kernel, cmap=plt.cm.RdBu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... to arrive at a result that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smooth_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ca54e4cd091d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvolve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhorizontal_edge_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizontal_edge_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizontal_edge_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRdBu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'smooth_image' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import convolve\n",
    "\n",
    "horizontal_edge_response = convolve(smooth_image, horizontal_edge_kernel)\n",
    "plt.imshow(horizontal_edge_response, cmap=plt.cm.RdBu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the variable names suggest, this filter highlights the horizontal edges of an image. We'll see what's happening here later on.\n",
    "\n",
    "(Note that the coloring in the kernel and the filtered image come from the colormap that's used---the output is still a grayscale image. The red just means that a value is negative, and the blue is positive.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An IPython widget for demonstrating image convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to develop an IPython widget that looks something like this:\n",
    "\n",
    "![Image of convolution widget](convolution_demo_widget.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slider in the widget allows you to step through the convolution process for each pixel in an image. The image (a white square with a black background) we use for the demo is really boring, just so the filtering process is clearer.\n",
    "\n",
    "The plot on the left shows the original, unfiltered, image. On top of that, we overlay the kernel position: The center pixel of the kernel is tinted red, and the remaining pixels in the kernel are tinted yellow. The red pixel is the one being altered by the current step of the convolution procedure, while red and yellow pixels are used to determine the replacement value.\n",
    "\n",
    "On the left, we see the image at the `i`th step of the convolution process, which gives the (partially) filtered result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started though, let's define some helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions are great: They make code much more readable and reusable, which is what we should all be striving for. It's not necessary to understand these functions right away. You can easily skip over this for now, and revisit it if you have questions about the actual widget implementation. The function names, themselves, should be enough to describe their... ahem... functionality (except for `iter_kernel_labels`, that one's tough to describe succintly)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over pixels with `iter_pixels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we're going to want to look at the individual pixels of an image. So, let's define an iterator (or actually a generator) to make that easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_pixels(image):\n",
    "    \"\"\" Yield pixel position (row, column) and pixel intensity. \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            yield (i, j), image[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"yields\" the row, column, and pixel value for each iteration of a loop. By the way: You wouldn't normally loop over pixels (since Python loops are a bit slow) but the whole point of this widget is to go step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing images side-by-side with `imshow_pair`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like I said, I like small utility functions, so I pulled out the code to plot side-by-side images into its own function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_pair(image_pair, titles=('', ''), figsize=(10, 5), **kwargs):\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "    for ax, img, label in zip(axes.ravel(), image_pair, titles):\n",
    "        ax.imshow(img, **kwargs)\n",
    "        ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with boundary conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the hardest part of any math problem (discrete, or otherwise)?\n",
    "\n",
    "Boundary conditions! (That's what they tell engineers, at least. If you're doing \"real\" math that's probably not true. Actually, even if that's not the case, it's probably not true.)\n",
    "\n",
    "There are many different solutions to dealing with boundaries; what we're going to do is just pad the input image with zeros based on the size of the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating border padding with `padding_for_kernel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define, a utility function to figure out how much padding to add based on the kernel shape. Basically, this just calculates the number of pixels that extend beyond the center pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_for_kernel(kernel):\n",
    "    \"\"\" Return the amount of padding needed for each side of an image.\n",
    "    \n",
    "    For example, of the returned result is [1, 2], then this means an \n",
    "    image should be padded with 1 extra row on top and bottom, and 2\n",
    "    extra columns on the left and right.\n",
    "    \"\"\"\n",
    "    # Slice to ignore RGB channels if they exist.\n",
    "    image_shape = kernel.shape[:2]\n",
    "    # We only want to handle kernels with odd dimensions so make sure that's true.\n",
    "    # (If a dimension is not odd, then the \"center\" pixel is a bit arbitrary.)\n",
    "    assert all((size % 2) == 1 for size in image_shape)\n",
    "    return [(size - 1) // 2 for size in image_shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding an image border with `add_padding`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define another utility funciton that uses the above function to pad the border of an image with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(image, kernel):\n",
    "    h_pad, w_pad = padding_for_kernel(kernel)\n",
    "    return np.pad(image, ((h_pad, h_pad), (w_pad, w_pad)),\n",
    "                  mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverse `add_padding` with `remove_padding`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sometimes, we need to take the padded image (or more likely, a filtered version of the padded image), and trim away the padded region, so we define a funtion to remove padding based on the kernel shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_padding(image, kernel):\n",
    "    inner_region = []  # A 2D slice for grabbing the inner image region\n",
    "    for pad in padding_for_kernel(kernel):\n",
    "        slice_i = slice(None) if pad == 0 else slice(pad, -pad)\n",
    "        inner_region.append(slice_i)\n",
    "    return image[inner_region]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make those functions a bit clearer, let's run through a demo. If you have an image that has a shape like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = np.empty((10, 20))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a kernel that has a shape like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = np.ones((3, 5))\n",
    "kernel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... adding padding to the image gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 24)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = add_padding(image, kernel)\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the total amount of padding is actually one less than the kernel size since you only need to add padding for *neighbors* of the center pixel, but not the center pixel, itself. (If this isn't clear, hopefully it will become clear when we start visualizing.)\n",
    "\n",
    "And of course, using `remove_padding` gives us the original shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_padding(padded, kernel).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing into the image with `window_slice`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to iterate over the pixels of an image and apply the convolution kernel in the 2D neighborhood (i.e. \"window\") of each pixel. To that end, it really helps to have an easy way to slice into an image based on the center of the kernel and the kernel shape, so here's a pretty simple way of doing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_slice(center, kernel):\n",
    "    r, c = center\n",
    "    r_pad, c_pad = padding_for_kernel(kernel)\n",
    "    # Slicing is (inclusive, exclusive) so add 1 to the stop value\n",
    "    return [slice(r-r_pad, r+r_pad+1), slice(c-c_pad, c+c_pad+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `center` parameter is just the (row, column) index corresponding to the center of the image patch where we'll be applying the convolution kernel.\n",
    "\n",
    "As a quick example, take a 2D array that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [10 11 12 13]\n",
      " [20 21 22 23]\n",
      " [30 31 32 33]]\n"
     ]
    }
   ],
   "source": [
    "image = np.arange(4) + 10 * np.arange(4).reshape(4, 1)\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `window_slice` to slice-out a 3x3 window of our array as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [10 11 12]\n",
      " [20 21 22]]\n"
     ]
    }
   ],
   "source": [
    "dummy_kernel = np.empty((3, 3))  # We only care about the shape\n",
    "center = (1, 1)\n",
    "print(image[window_slice(center, dummy_kernel)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the center pixel is 11, which corresponds to row 1, column 1 of the original array. We can increment the column to shift to the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [11 12 13]\n",
      " [21 22 23]]\n"
     ]
    }
   ],
   "source": [
    "print(image[window_slice((1, 2), dummy_kernel)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or increment the row to shift down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 11 12]\n",
      " [20 21 22]\n",
      " [30 31 32]]\n"
     ]
    }
   ],
   "source": [
    "print(image[window_slice((2, 1), dummy_kernel)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-squre kernels would work too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11]\n",
      " [21]\n",
      " [31]]\n"
     ]
    }
   ],
   "source": [
    "dummy_kernel = np.empty((3, 1))\n",
    "print(image[window_slice((2, 1), dummy_kernel)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the kernel to an image patch with `apply_kernel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually \"apply\" the convolution kernel to an image patch, we just grab an image patch based on the center location and the kernel shape, and then \"apply\" the kernel by taking the sum of pixel intensities under the kernel, weighted by the kernel values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kernel(center, kernel, original_image):\n",
    "    image_patch = original_image[window_slice(center, kernel)]\n",
    "    # An element-wise multiplication followed by the sum (i.e. a weighted average)\n",
    "    return np.sum(kernel * image_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, convolution requires flipping the kernel horizontally and vertically, but that's not really an important detail here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking where the kernel is located with `iter_kernel_labels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole point of this widget is to visualize how convolution works, so we need a way to display where the convolution kernel is located at any given iteration. To that end, we do a bit of array manipulation to mark:\n",
    "* Pixels *under* the kernel with a value of 1\n",
    "* The pixel at the center of the kernel with a value of 2\n",
    "* All other pixels with a value of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_kernel_labels(image, kernel):\n",
    "    \"\"\" Yield position and kernel labels for each pixel in the image.\n",
    "\n",
    "    The kernel label-image has a 2 at the center pixel and 1 for every other\n",
    "    pixel \"under\" the kernel. Pixels not under the kernel are labeled as 0.\n",
    "    \n",
    "    Note that the mask is the same size as the input image.\n",
    "    \"\"\"\n",
    "    original_image = image\n",
    "    image = add_padding(original_image, kernel)\n",
    "    i_pad, j_pad = padding_for_kernel(kernel)\n",
    "    for (i, j), pixel in iter_pixels(original_image):\n",
    "        # We padded the image so this shifts the center to the *original* location\n",
    "        i += i_pad\n",
    "        j += j_pad\n",
    "        mask = np.zeros(image.shape, dtype=int)  # Background = 0\n",
    "        mask[window_slice((i, j), kernel)] = 1   # Kernel = 1\n",
    "        mask[i, j] = 2                           # Kernel-center = 2\n",
    "        yield (i, j), mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our kernel overlay with `visualize_kernel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to take those 1s and 2s marking our kernel, and turn that into a color overlay. We do that using a little utility from scikit-image that overlays label values onto an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color\n",
    "\n",
    "def visualize_kernel(kernel_labels, image):\n",
    "    \"\"\" Return an overlay image, where 1's will be yellow and 2's will be red.\n",
    "    \n",
    "    See `iter_kernel_labels` for info on the meaning of 1 and 2.\n",
    "    \"\"\"\n",
    "    return color.label2rgb(kernel_labels, image, bg_label=0,\n",
    "                           colors=('yellow', 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we mark the center value (2) as red and neighboring values (1) as yellow. The background value (0) is transparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython widget demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all the above helper functions were just to get us to this point: Making our own IPython widget.\n",
    "\n",
    "But before that (such a tease), here's a *really* basic example of IPython widgets, in case the concept is completely new to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A very simple widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define your own IPython widget, all you need to do is pass a function and the argument(s) you want to control to `widgets.interact`. So a very simple example would just be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80d7fa5f15948b381e6211da35faac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='i', max=10), Output()), _dom_classes=('widget-interact',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets\n",
    "\n",
    "def printer(i):\n",
    "    print(\"i = {}\".format(i))\n",
    "\n",
    "ipywidgets.interact(printer, i=(0, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving the slider changes the value printed by `printer`. The keyword argument, `i`, must match the argument name in `printer`; that's how slider value gets connected to the `printer` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A stepper function for image convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the real widget, we're going to combine all of the helper functions defined above. Unfortunately, there're a couple of things here that make the code a bit more complicated than I would like:\n",
    "\n",
    "* First, I wanted to make something that's fairly reusable. To that end, the following code snippet creates a function that *returns* the function passed to `widgets.interact`. That way we can prep the image and cache results (see below).\n",
    "  - This function-that-returns-a-function is called a closure. Here's a pretty good explanation of the concept: [Closure explanation](http://stackoverflow.com/a/141426/260303)\n",
    "\n",
    "* I'm going to do a bit of work here to cache results so that the demo function only computes the filtered result for each pixel once. Basically, we iterate over pixels in order, so we can cache a result for a pixel, and then we reuse the result to compute the result for the next pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_convolution_step_function(image, kernel, **kwargs):\n",
    "    # Initialize generator since we're only ever going to iterate over a pixel\n",
    "    # once, even if we step back.\n",
    "    gen_kernel_labels = iter_kernel_labels(image, kernel)    \n",
    "    \n",
    "    image_cache = []\n",
    "    image = add_padding(image, kernel)\n",
    "    \n",
    "    def convolution_step(i_step):\n",
    "        \"\"\" Plot the original image and kernel-overlay next to the filtered image.\n",
    "        \n",
    "        For a given step, check if it's in the image cache. If not calculate all\n",
    "        necessary images. Then plot the requested step result.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create all images up to the current step, unless they're already cached:\n",
    "        while i_step >= len(image_cache):\n",
    "            \n",
    "            # For the first step (`i_step == 0`), the original image is the filtered image;\n",
    "            # after that we look in the cache, which stores (`kernel_overlay`, `filtered`).\n",
    "            filtered_prev = image if i_step == 0 else image_cache[-1][1]\n",
    "            # We don't want to overwrite the previously filtered image:\n",
    "            filtered = filtered_prev.copy()\n",
    "            \n",
    "            # Get the labels used to visualize the kernel\n",
    "            center, kernel_labels = gen_kernel_labels.next()\n",
    "            # Modify the pixel value at the kernel center\n",
    "            filtered[center] = apply_kernel(center, kernel, image)\n",
    "            # Take the original image and overlay our kernel visualization\n",
    "            kernel_overlay = visualize_kernel(kernel_labels, image)\n",
    "            # Save images for reuse.\n",
    "            image_cache.append((kernel_overlay, filtered))\n",
    "\n",
    "        # Before displaying, remove the padding we added to deal with boundary conditions\n",
    "        # (Loop since each step has an original-image/kernel-verlay and a filtered image)\n",
    "        image_pair = [remove_padding(each, kernel) for each in image_cache[i_step]]\n",
    "        imshow_pair(image_pair, **kwargs)\n",
    "        plt.show()\n",
    "        \n",
    "    return convolution_step  # <-- this is a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just initialize the stepper function and pass that to `widgets.interact`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_convolution_demo(image, kernel, **kwargs):\n",
    "    convolution_step = make_convolution_step_function(image, kernel, **kwargs)\n",
    "    step_slider = widgets.IntSliderWidget(min=0, max=image.size-1, value=0)\n",
    "    ipywidgets.interact(convolution_step, i_step=step_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a bit of tweaking here just to get the slider widget to start off at zero, but that's not crucial. You could have used\n",
    "```\n",
    "widgets.interact(convolution_step, i_step=(0, image.size-1))\n",
    "```\n",
    "but that would start with the slider at the midpoint, which isn't ideal for this particular demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Mean filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using this widget, let's define a really small image, which makes this demo easier to understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bright_square = np.zeros((7, 7), dtype=float)\n",
    "bright_square[2:5, 2:5] = 1\n",
    "plt.imshow(bright_square);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the classic smoothing filters is the mean filter. As you might expect, it calculates the mean under the kernel. The kernel itself is just the weights used for the mean. For 3x3 kernel, this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11111111 0.11111111 0.11111111]\n",
      " [0.11111111 0.11111111 0.11111111]\n",
      " [0.11111111 0.11111111 0.11111111]]\n"
     ]
    }
   ],
   "source": [
    "mean_kernel = np.ones((3, 3), dtype=float)\n",
    "mean_kernel /= mean_kernel.size\n",
    "print(mean_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights will then be multiplied by pixel intensities using `apply_kernel`.\n",
    "\n",
    "Using our convolution demo-widget, we can see how mean-filtering process looks, step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "IntSliderWidget",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/importstring.py\u001b[0m in \u001b[0;36mimport_item\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mpak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'ipywidgets' has no attribute 'IntSliderWidget'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/shimmodule.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimport_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/importstring.py\u001b[0m in \u001b[0;36mimport_item\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No module named %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named IntSliderWidget",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4bfed49ca8a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Image and kernel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Filtered image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m interactive_convolution_demo(bright_square, mean_kernel, \n\u001b[0;32m----> 3\u001b[0;31m                              vmax=1, titles=titles)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-35838269ba65>\u001b[0m in \u001b[0;36minteractive_convolution_demo\u001b[0;34m(image, kernel, **kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minteractive_convolution_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mconvolution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_convolution_step_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstep_slider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntSliderWidget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mipywidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvolution_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_slider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/shimmodule.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimport_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: IntSliderWidget"
     ]
    }
   ],
   "source": [
    "titles = ('Image and kernel', 'Filtered image')\n",
    "interactive_convolution_demo(bright_square, mean_kernel, \n",
    "                             vmax=1, titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boundary conditions, revisited:** If you look at `i_step = 0`, you can see why we went through the trouble of defining all that image-padding code: If we want to apply the convolution kernel to the top-left pixel, it has no neighbors above it or to the left. Adding padding (which was removed for display) allows us to handle those cases without too much trouble.\n",
    "\n",
    "After playing around with the widget, you should notice that the mean kernel is really simple: \n",
    "- Weight each pixel under the kernel (red+yellow) equally\n",
    "- Add all products (pixel-values × 1/9) together\n",
    "- Replace center pixel (red) with the sum\n",
    "\n",
    "In the filtered result, hard edges are smoothed: Since a pixel on an edge will be bordering both white and black pixels, the filtered result will be gray. This smoothing effect can be useful for blurring an image or removing noise (although [edge-preserving denoising filters](http://scikit-image.org/docs/dev/auto_examples/plot_denoise.html) are probably preferable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Edge filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at another really useful and easy-to-understand filter: The edge filter. For images, edges are basically boundaries between light and dark values. An easy way to calculate that is to take the the difference of neighboring values.\n",
    "\n",
    "Here, we'll use the Sobel kernel for detecting horizontal edges (which was defined at the very beginning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  1]\n",
      " [ 0  0  0]\n",
      " [-1 -2 -1]]\n"
     ]
    }
   ],
   "source": [
    "print(horizontal_edge_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, using this kernel to calculate a weighted sum will subtract neighboring values *below* the center pixel from those *above* the center. If pixels above and below the center are the same, the filtered result is 0, but if they are very different, we get a strong \"edge\" response.\n",
    "\n",
    "Again, using our convolution widget, we can step through the process quite easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkUAAAEaCAYAAAAfTiN+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nACRRigAAkihFAABJkv8DUKbI2Pkc3GkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dec16d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_convolution_demo(bright_square, horizontal_edge_kernel,\n",
    "                             vmin=-4, vmax=4, cmap=plt.cm.RdBu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the widget a bit. You should notice that:\n",
    "- This filter responds to horizontal edges (i.e. it is sensitive to the orientation of the edge)\n",
    "- The filter responds differently when going from white-to-black vs black-to-white (i.e. it is sensitive to the direction of the edge)\n",
    "- The edge response diminishes as it approaches a vertical boundary\n",
    "\n",
    "Often, you don't really care about the orientation or direction of the edge. In that case, you would just combine the horizontal-edge filter with the corresponding vertical-edge filter and calculate the gradient magnitude. This is exactly what the standard [Sobel filter](http://scikit-image.org/docs/dev/auto_examples/plot_edge_filter.html) does.\n",
    "\n",
    "I hope that clarifies the idea of convolution filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leftovers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q: Why did the first example use a `gaussian_filter`?\n",
    "  - A: Edge filters (which are basically just derivatives) enhance noise. We do some smoothing beforehand to reduce the likelihood of false edges.\n",
    "* Q: Why are the \"edges\" (the red and blue regions) in that last filtered image so thick?\n",
    "  - A: The edge filter used here gives what's called a \"centered difference\". In reality, the edges lie in-between pixel values, so the closest we can get (without biasing the edge up or down) is to mark the pixels above and below the edge.\n",
    "* Q: What do you mean by \"neighbors\" and \"under\" the kernel?\n",
    "  - A: In the kernel overlay, red marks the center pixel, yellow marks the neighbors, and both red and yellow pixels are \"under\" the kernel.\n",
    "* Really useful linear filters:\n",
    "  - Gaussian filter <http://scikit-image.org/docs/dev/api/skimage.filter.html#gaussian-filter>, the classic smoothing filter\n",
    "  - Sobel filter http://scikit-image.org/docs/dev/auto_examples/plot_edge_filter.html, for detection edges (a little different from the above since it takes the gradient magnitude, which means it doesn't care about direction or orientation)\n",
    "* Useful generic filters:\n",
    "  - Generic (local) filters work in a similar fashion to the convolution filters described above, but they aren't limited to a linear, weighted sum.\n",
    "  - Denoising filters http://scikit-image.org/docs/dev/auto_examples/plot_denoise.html, for removing noise without smoothing edges\n",
    "  - Rank (e.g. min, median, mode, max) filters http://scikit-image.org/docs/dev/auto_examples/applications/plot_rank_filters.html\n",
    "  - Morphological filter http://scikit-image.org/docs/dev/auto_examples/applications/plot_morphology.html, for manipulating shapes\n",
    "* Scipy 2014 tutorial on scikit-image http://tonysyu.github.io/scikit-image-tutorial-at-scipy-2014.html\n",
    "* This article in notebook form: [Download Notebook](http://tonysyu.github.io/includes/Image_convolution_demo.ipynb) (so you can actually use the widgets.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
